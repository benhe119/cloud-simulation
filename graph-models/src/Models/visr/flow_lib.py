import histogram
import ipaddress

def ip2long(ip):
    """ 
    Convert an IP string to long
    """
    #packedIP = socket.inet_aton(ip)
    #return struct.unpack("!L", packedIP)[0]
    return int(ipaddress.ip_address(ip))

def long2ip(num):
    """ 
    Convert an IP string to long
    """
    return str(ipaddress.ip_address(num))

def median(mylist):
    list_sz = len(mylist)
    if list_sz == 0:
        print('empty list!')
    if list_sz == 1:
        return mylist[0] 
    sorts = sorted(mylist)
    length = len(sorts)
    mid = int(length/2)
    #if not length % 2:
        #return (sorts[mid] + sorts[mid - 1]) / 2.0
    return sorts[mid]

class Flow:
    def __init__(self):
        self.flow_tuple = ['', '', '', '', '']
        self.num_bytes = 0
        self.num_src_bytes = 0
        self.num_dst_bytes = 0
        self.duration = 0.0
        self.user = ''
        return

    def set_params(self, src_ip, dst_ip, src_port, dst_port, protocol):
        self.flow_tuple[0] = src_ip
        self.flow_tuple[1] = dst_ip
        self.flow_tuple[2] = src_port
        self.flow_tuple[3] = dst_port
        self.flow_tuple[4] = protocol
        return

    def set_user(self, user):
        self.user = user
        return

    def get_ip(self, get_src):
        if get_src:
            return self.flow_tuple[0]
        else:
            return self.flow_tuple[1]
    
    def get_subnet(self, get_src, group_quads):
        ip = self.get_ip(get_src)
        #pos = ip.rfind('.')
        #return ip[0:pos]
        quads = ip.split('.')
        return '.'.join(quads[0:group_quads])

    def get_num_bytes(self):
        return int(self.num_bytes)

    def get_duration(self):
        return float(self.duration)

    def set_flow_metadata(self, num_bytes, duration):
        self.num_bytes = num_bytes
        self.duration = duration
        return

    def set_num_bytes(self, num_src_bytes, num_dst_bytes):
        self.num_src_bytes = num_src_bytes
        self.num_dst_bytes = num_dst_bytes
        return

    def get_src_bytes(self):
        return int(self.num_src_bytes)

    def get_dst_bytes(self):
        return int(self.num_src_bytes)

    def map(self, use_protocol=False, key_sep=' '):
        key = ''
        if use_protocol:
            key = key_sep.join(self.flow_tuple)
        else:
            key = key_sep.join(self.flow_tuple[0:1])
        value = num_bytes
        return [key, value]

    def get_user(self):
        return self.user

def parse_event_log(line):
    """ 
    Function to parse windows event logs data.
    """
    if line[0] != 'a':
        return None
    
    t = line.split(' ')
    flow = Flow()
    #flow.set_params(long2ip(t[1]), long2ip(t[2]), t[12], '0', t[5])
    if len(t) < 13:
        #print(line)
        return None
    #else:
        #print(len(t))
    flow.set_params(t[1], t[2], t[12], '0', t[5])
    flow.set_user(t[8])
    return flow

def parse_silk_flow(line):
    """ 
    Function to parse flow files generated by the pcap2netflow script using Silk.
    """
    t = line.split('|')
    for i in range(len(t)):
        t[i] = t[i].strip()
    if t[0][0] == 's':
        return None
    if t[0] == '255.255.255.255' or t[1] == '255.255.255.255':
        return None 
    if t[0] == '0.0.0.0' or t[1] == '0.0.0.0':
        return None 

    flow = Flow()
    flow.set_params(t[0], t[1], t[2], t[3], t[4])
    flow.set_flow_metadata(t[6], t[9])
    return flow

def parse_flo(line):
    """ 
    Function to parse flow files generated by the pcap2netflow script using Silk.
    """
    t = line.split(',')
    src = t[5]
    dst = t[11]
    if src == '255.255.255.255' or dst == '255.255.255.255':
        return None 
    if src == '0.0.0.0' or dst == '0.0.0.0':
        return None 

    flow = Flow()
    src_port = t[17]
    dst_port = t[18]
    protocol = t[4]
    flow.set_params(src, dst, src_port, dst_port, protocol)
    duration = t[21]
    num_bytes = int(t[22]) + int(t[23])
    num_src_bytes = t[22]
    num_dst_bytes = t[23]
    #num_src_packet_count = t[26]
    #num_dst_packet_count = t[27]
    flow.set_flow_metadata(num_bytes, duration)
    flow.set_num_bytes(num_src_bytes, num_dst_bytes)
    return flow

def process_flow_data(path, flow_parser, flow_processor):
    """ 
    Generic function for line by line processing of a flow file.
    flow_parser is a function/object that takes a line and returns 
    a Flow object.  flow_process takes a Flow object and performs
    tasks such as aggregation/feature extraction.
    """
    print('Processing [' + path + ']')
    f_in = open(path)
    line_count = 1
    for line in f_in:
        line = line.strip()
        flow = flow_parser(line)
        if flow == None:
            continue
        flow_processor(flow)
    f_in.close()
    return

class HashedIdMapper:
    def __init__(self, offset=0):
        self.id_map = dict()
        self.rev_mapping = dict()
        self.offset = offset

    #def load(self, csv_file):
        #fin = open(csv_file)
        #for line in fin:
            #t = line.strip().split(',')
            #self.id_map[t[0]] = t[1]
        #fin.close()
#
    #def reset(self):
        #self.id_map.clear()
    
    def map(self, key):
        if key in self.id_map:
            id = self.id_map[key]
        else:
            id = str(self.offset + len(self.id_map))
            self.id_map[key] = id
            self.rev_mapping[id] = key
        return id
    
    def rev_map(self, id):
        if id not in self.rev_mapping:
            return ''
        return self.rev_mapping[id]

    def store(self, outpath):
        fout = open(outpath, 'w')
        for k,v in self.id_map.items():
            fout.write(k + ',' + v + '\n')
        fout.close()

class AggregatedWeightedTsvWriter:
    def __init__(self):
        self.edge_counts = dict()

    def add_edge(self, u, v):
        key = u + ' ' + v
        if key in self.edge_counts:
            self.edge_counts[key] += 1
        else:
            self.edge_counts[key] = 1

    def flush(self, out_path):
        f = open(out_path, 'w')
        for key in self.edge_counts:
            f.write(key + ' ' + str(self.edge_counts[key]) + '\n')
        f.close()

class AggregatedTsvWriter:
    def __init__(self):
        self.edges = set()

    def add_edge(self, u, v):
        key = u + ' ' + v
        self.edges.add(key)

    def flush(self, out_path):
        f = open(out_path, 'w')
        for key in self.edges:
            f.write(key + '\n')
        f.close()

class FlowGraphMapper:
    def __init__(self):
        self.tsv_writer = AggregatedTsvWriter()
        self.id_mapper = HashedIdMapper()
        return
    
    def transform_flow(self, flow):
        #u = str(ip2long(flow.get_ip(True)))
        u = self.id_mapper.map(flow.get_ip(True))
        #if u == '1122729524':
            #print(flow.get_ip(True))
        #v = str(ip2long(flow.get_ip(False)))
        v = self.id_mapper.map(flow.get_ip(False))
        #if v == '1122729524':
            #print(flow.get_ip(False))
        self.tsv_writer.add_edge(u, v)
        return

    def get_id_mapper(self):
        return self.id_mapper

    def store_graph(self, outpath):
        print('Writing graph to: ' + outpath)
        self.tsv_writer.flush(outpath)
        return

    def store_mapping(self, outpath):
        print('Writing IP-id map to: ' + outpath)
        self.id_mapper.store(outpath)

class FlowStatisticsCollector:
    def __init__(self, monitor_flow, accumulate_total_flow=True):
        self.flow_sizes = []
        self.flow_durations = []
        self.flow_bins = []
        self.in_flows = dict()
        self.out_flows = dict()
        self.in_median_flows = dict()
        self.out_median_flows = dict()
        self.monitor_flow = monitor_flow
        if accumulate_total_flow:
            print('#  Initializing flow statistics collector to \
                    monitor bi-directional flow sizes')
        self.accumulate_total_flow = accumulate_total_flow
        return
    
    def accumulate_flow_sizes(self, flow):
        #flow_sz = flow.get_num_bytes()
        if self.monitor_flow:
            if self.accumulate_total_flow:
                flow_size = flow.get_num_bytes()
                self.flow_sizes.append(flow_size)
            else:
                src_flow_size = flow.get_src_bytes()
                dst_flow_size = flow.get_dst_bytes()
                self.flow_sizes.append(src_flow_size)
                self.flow_sizes.append(dst_flow_size)
        else:
            flow_duration = flow.get_duration()
            self.flow_durations.append(flow_duration)
        return

    def bin_flows(self):
        if self.monitor_flow:
            self.flow_sizes.append(0)
            self.flow_bins = histogram.equi_depth_histogram(self.flow_sizes, 5)
        else:
            self.flow_bins = histogram.equi_depth_histogram(self.flow_durations, 5)
        print(self.flow_bins)
        return 

    def accumulate_binned_flow_sizes(self, flow):
        src_ip = flow.get_ip(True)
        dst_ip = flow.get_ip(False)

        if src_ip not in self.out_flows:
            self.out_flows[src_ip] = []
            self.in_flows[src_ip] = []
        if dst_ip not in self.in_flows:
            self.in_flows[dst_ip] = []
            self.out_flows[dst_ip] = []

        if self.monitor_flow:
            if self.accumulate_total_flow:
                flow_id = histogram.get_feature(self.flow_bins, flow.get_num_bytes())
                self.out_flows[src_ip].append(flow_id)
                self.in_flows[dst_ip].append(flow_id)
            else:
                src_flow_id = histogram.get_feature(self.flow_bins, flow.get_src_bytes())
                dst_flow_id = histogram.get_feature(self.flow_bins, flow.get_dst_bytes())
                self.out_flows[src_ip].append(src_flow_id)
                self.in_flows[dst_ip].append(src_flow_id)
                self.out_flows[dst_ip].append(dst_flow_id)
                self.in_flows[src_ip].append(dst_flow_id)
        else:
            flow_id = histogram.get_feature(self.flow_bins, flow.get_duration())
            self.out_flows[src_ip].append(flow_id)
            self.in_flows[dst_ip].append(flow_id)
        return

    def find_median_flows(self):
        ip_set = set()
        for ip in self.in_flows.keys():
            flow_values = self.in_flows[ip] 
            if len(flow_values) > 0:
                self.in_median_flows[ip] = median(flow_values)
            ip_set.add(ip)
        for ip in self.out_flows.keys():
            flow_values = self.out_flows[ip]
            if len(flow_values) > 0:
                self.out_median_flows[ip] = median(flow_values)
            ip_set.add(ip)
        for ip in ip_set:
            if ip not in self.in_median_flows:
                self.in_median_flows[ip] = histogram.get_feature(self.flow_bins, 0)
            if ip not in self.out_median_flows:
                self.out_median_flows[ip] = histogram.get_feature(self.flow_bins, 0)
        print('# inflows = ' + str(len(self.in_median_flows)))
        print('# outflows = ' + str(len(self.out_median_flows)))

    def get_median_flows(self, get_in_flow, out_dict):
        if get_in_flow:
            for k,v in self.in_median_flows.items():  
                out_dict[k] = v
        else:
            for k,v in self.out_median_flows.items():
                out_dict[k] = v

def get_median_flow(path, median_in_flow, median_out_flow, monitor_flow):
    flow_stats_collector = FlowStatisticsCollector(monitor_flow, False)
    process_flow_data(path, parse_flo, \
            flow_stats_collector.accumulate_flow_sizes)
    flow_stats_collector.bin_flows()
    process_flow_data(path, parse_flo, \
            flow_stats_collector.accumulate_binned_flow_sizes)
    flow_stats_collector.find_median_flows()
    flow_stats_collector.get_median_flows(True, median_in_flow)
    flow_stats_collector.get_median_flows(False, median_out_flow)
    print('# inflows = ' + str(len(median_in_flow)))
    print('# outflows = ' + str(len(median_out_flow)))
    return
